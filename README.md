# RL

2-armed Bandit problem:

Think of an agent that plays a 2-armed bandit, trying to maximize its total reward. In each step, the 
# agent selects one of the levers and is given some reward according to the reward distribution of that 
# lever. Assume that reward distribution for the first lever is a Gaussian with ğœ‡1 = 5, ğœ12 = 10, and for 
# the second lever is a binomial Gaussian with ğœ‡21 = 10, ğœ21^2 = 15, ğœ‡22 = 4, ğœ22^2 = 10, which means 
# that  the  resulting  output  will  be  uniformly  probable  from  these  two  Gaussian  distributions. 
# In  this  problem,  we  assume  the  reward  distributions  are  unknown,  and  the  agent  only  
# sees  a  realization  of  reward  after  selecting  an  action.  The  agent  takes  action  according  to  the  ğœ–ğœ–-
# greedy action selection policy with parameter ğœ–. We  consider  the  agent  selects  1000  actions,  which  is  referred  to  as  step/time.  In  order  to  have  
# smooth results, we repeat 1000 steps for 100 independent runs. 

# Part a - In this part, set the initial Q values at the beginning of each run as ğ‘„(ğ‘1) = ğ‘„(ğ‘2) = 0. Assuming 
# action ğ‘ is  selected  at  time  step  ğ‘˜ and  the  reward  ğ‘Ÿ_ğ‘˜  is  observed,  the  Q-value  for  the  
# corresponding  action  will  be  updated  according  to:  ğ‘„(ğ‘) = ğ‘„(ğ‘) + ğ›¼( r âˆ’ ğ‘„(ğ‘) ).  For  the  
# learning rates, consider the following values: ğ›¼ = 1, ğ›¼ = 0.9ğ‘˜ , ğ›¼ =1/1+Ln(1+ğ‘˜) and ğ›¼ = 1/ğ‘˜ and for 
# the  ğœ–-greedy  policy,  use  ğœ– = 0, 0.1, 0.2, 0.5.  
# For the ğ‘–ğ‘–th independent run, you need to keep track of accumulated rewards as: 
# Acc R^i_k=(1/k) sum_{j=1 to k} r_j
# where Acc R^i_k denotes the average reward per step obtained by the agent up to the time step ğ‘˜ in  the  ith  independent  run.  Then  the  average  over  100  independent  runs  of  accumulated 
# rewards Acc R^i_k can be obtained at any given step/time ğ‘˜ = 1, ... ,1000 as: 
# Acc R_k=(1/100) sum_{j=1 to 100} Acc R^i_k

# Part b - For  a  fixed  ğ›¼ = 0.1  and  ğœ–= 0.1,  use  the  following  optimistic  initial  values  and  compare  the  
# results:  ğ‘„ = [0 0],  ğ‘„  = [5 7], ğ‘„ = [20 20]  (note  that  ğ‘„ = [ğ‘„(ğ‘1)  ğ‘„(ğ‘2)]).

# Part c - For a fixed ğ›¼ = 0.1, use the Gradient-Bandit policy with ğ»1(ğ‘1) = ğ»1(ğ‘2) = 0. Plot the average 
# accumulated  reward  with  respect  to  step/time.  How  the  results  are  different  from  ğœ–-greedy 
#results with ğ‘„(ğ‘1) = ğ‘„(ğ‘2) = 0, ğ›¼ğ›¼ = 0.1 and ğœ– = 0.1? 
